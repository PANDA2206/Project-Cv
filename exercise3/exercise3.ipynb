{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.984627Z",
     "start_time": "2023-12-06T16:19:47.900374Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for python3: read in python2 pickled files\n",
    "import _pickle as cPickle\n",
    "\n",
    "import gzip\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import cv2\n",
    "from parmap import parmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import progressbar\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.985440Z",
     "start_time": "2023-12-06T16:19:47.911436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def spawn(f):\n",
    "    def fun(q_in, q_out):\n",
    "        while True:\n",
    "            i,x = q_in.get()\n",
    "            if i is None:\n",
    "                break\n",
    "            q_out.put((i, f(x)))\n",
    "\n",
    "    return fun"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.994282Z",
     "start_time": "2023-12-06T16:19:47.919143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def parmap(f, iterable, nprocs=multiprocessing.cpu_count(),\n",
    "           show_progress=False, size=None):\n",
    "    \"\"\"\n",
    "    @param f\n",
    "    function to be applied to the items in iterable\n",
    "    @param iterable\n",
    "    ...\n",
    "    @param nprocs\n",
    "    number of processes\n",
    "    @param show_progress\n",
    "    True <-> show a progress bar\n",
    "    @param size\n",
    "    number of items in iterable.\n",
    "    If show_progress == True and size is None and iterable is not already a\n",
    "    list, it is converted to a list first. This could be bad for generators!\n",
    "    (If size is not needed right away for the progress bar, all input items\n",
    "    are enqueued before reading the results from the output queue.)\n",
    "    TLDR: If you know it, tell us the size of your iterable.\n",
    "    \"\"\"\n",
    "    q_in = multiprocessing.Queue(1)\n",
    "    q_out = multiprocessing.Queue()\n",
    "\n",
    "    progress = None\n",
    "    if show_progress:\n",
    "        if not isinstance(iterable, list):\n",
    "            iterable = list(iterable)\n",
    "        size = len(iterable)\n",
    "\n",
    "        widgets = [ progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA() ]\n",
    "        progress = progressbar.ProgressBar(widgets=widgets, maxval=size)\n",
    "\n",
    "    proc = [multiprocessing.Process(target=spawn(f), args=(q_in, q_out)) for _ in range(nprocs)]\n",
    "\n",
    "    for p in proc:\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "\n",
    "    if progress is not None:\n",
    "        progress.start()\n",
    "\n",
    "\n",
    "    def enqueue():\n",
    "        s = 0\n",
    "        for i, x in enumerate(iterable):\n",
    "            q_in.put((i,x))\n",
    "            s += 1\n",
    "\n",
    "        for _ in range(nprocs):\n",
    "            q_in.put((None,None))\n",
    "\n",
    "        return s\n",
    "\n",
    "    pool = ThreadPool(processes=1)\n",
    "    async_size = pool.apply_async(enqueue)\n",
    "\n",
    "    if size is None:\n",
    "        # this is the old behavior\n",
    "        size = async_size.get()\n",
    "\n",
    "    res = []\n",
    "    progress_value = 0\n",
    "    for _ in range(size):\n",
    "        r = q_out.get()\n",
    "        res.append(r)\n",
    "\n",
    "        # we could: insert sorted, yield all results we have so far\n",
    "\n",
    "        if progress is not None:\n",
    "            progress_value += 1\n",
    "            progress.update(progress_value)\n",
    "\n",
    "    del pool\n",
    "    for p in proc:\n",
    "        p.join()\n",
    "\n",
    "    if progress is not None:\n",
    "        progress.finish()\n",
    "\n",
    "    return [ x for _, x in sorted(res) ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.995109Z",
     "start_time": "2023-12-06T16:19:47.923194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def parseArgs(parser):\n",
    "    parser.add_argument('--labels_test',\n",
    "                        help='contains test images/descriptors to load + labels')\n",
    "    parser.add_argument('--labels_train',\n",
    "                        help='contains training images/descriptors to load + labels')\n",
    "    parser.add_argument('-s', '--suffix',\n",
    "                        default='_SIFT_patch_pr.pkl.gz',\n",
    "                        help='only chose those images with a specific suffix')\n",
    "    parser.add_argument('--in_test',\n",
    "                        help='the input folder of the test images / features')\n",
    "    parser.add_argument('--in_train',\n",
    "                        help='the input folder of the training images / features')\n",
    "    parser.add_argument('--overwrite', action='store_true',\n",
    "                        help='do not load pre-computed encodings')\n",
    "    parser.add_argument('--powernorm', action='store_true',\n",
    "                        help='use powernorm')\n",
    "    parser.add_argument('--gmp', action='store_true',\n",
    "                        help='use generalized max pooling')\n",
    "    parser.add_argument('--gamma', default=1, type=float,\n",
    "                        help='regularization parameter of GMP')\n",
    "    parser.add_argument('--C', default=1000, type=float,\n",
    "                        help='C parameter of the SVM')\n",
    "    return parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.995290Z",
     "start_time": "2023-12-06T16:19:47.940618Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def getFiles(folder, pattern, labelfile):\n",
    "    \"\"\"\n",
    "    returns files and associated labels by reading the labelfile\n",
    "    parameters:\n",
    "        folder: inputfolder\n",
    "        pattern: new suffix\n",
    "        labelfiles: contains a list of filename and labels\n",
    "    return: absolute filenames + labels\n",
    "    \"\"\"\n",
    "    # read labelfile\n",
    "    with open(labelfile, 'r') as f:\n",
    "        all_lines = f.readlines()\n",
    "\n",
    "    # get filenames from labelfile\n",
    "    all_files = []\n",
    "    labels = []\n",
    "    check = True\n",
    "    for line in all_lines:\n",
    "        # using shlex we also allow spaces in filenames when escaped w. \"\"\n",
    "        splits = shlex.split(line)\n",
    "        file_name = splits[0]\n",
    "        class_id = splits[1]\n",
    "\n",
    "        # strip all known endings, note: os.path.splitext() doesnt work for\n",
    "        # '.' in the filenames, so let's do it this way...\n",
    "        for p in ['.pkl.gz', '.txt', '.png', '.jpg', '.tif', '.ocvmb','.csv']:\n",
    "            if file_name.endswith(p):\n",
    "                file_name = file_name.replace(p,'')\n",
    "\n",
    "        # get now new file name\n",
    "        true_file_name = os.path.join(folder, file_name + pattern)\n",
    "        all_files.append(true_file_name)\n",
    "        labels.append(class_id)\n",
    "\n",
    "    return all_files, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.995405Z",
     "start_time": "2023-12-06T16:19:47.945417Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1519255417.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[21], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    = all_files\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:19:47.995932Z",
     "start_time": "2023-12-06T16:19:47.950397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def loadRandomDescriptors(files, max_descriptors):\n",
    "    \"\"\"\n",
    "    load roughly `max_descriptors` random descriptors\n",
    "    parameters:\n",
    "        files: list of filenames containing local features of dimension D\n",
    "        max_descriptors: maximum number of descriptors (Q)\n",
    "    returns: QxD matrix of descriptors\n",
    "    \"\"\"\n",
    "    # let's just take 100 files to speed-up the process\n",
    "    max_files = 100\n",
    "    indices = np.random.permutation(max_files)\n",
    "    files = np.array(files)[indices]\n",
    "\n",
    "    # rough number of descriptors per file that we have to load\n",
    "    max_descs_per_file = int(max_descriptors / len(files))\n",
    "\n",
    "    descriptors = []\n",
    "    for i in tqdm(range(len(files))):\n",
    "        with gzip.open(files[i], 'rb') as ff:\n",
    "            # for python2\n",
    "            # desc = cPickle.load(ff)\n",
    "            # for python3\n",
    "            desc = cPickle.load(ff, encoding='latin1')\n",
    "\n",
    "        # get some random ones\n",
    "        indices = np.random.choice(len(desc),\n",
    "                                   min(len(desc),\n",
    "                                       int(max_descs_per_file)),\n",
    "                                   replace=False)\n",
    "        desc = desc[indices]\n",
    "        descriptors.append(desc)\n",
    "\n",
    "    descriptors = np.concatenate(descriptors, axis=0)\n",
    "    return descriptors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:20:05.931627Z",
     "start_time": "2023-12-06T16:20:05.882272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def dictionary(descriptors, n_clusters):\n",
    "    \"\"\"\n",
    "    return cluster centers for the descriptors\n",
    "    parameters:\n",
    "        descriptors: NxD matrix of local descriptors\n",
    "        n_clusters: number of clusters = K\n",
    "    returns: KxD matrix of K clusters\n",
    "    \"\"\"\n",
    "    # kmeans = MiniBatchKMeans(n_clusters,batch_size= 10000,random_state = 100)\n",
    "    # kmeans.fit(descriptors)\n",
    "    # codebook = kmeans.cluster_centers_\n",
    "\n",
    "    # TODO\n",
    "    # Use MiniBatchKMeans to create the codebook\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=100, batch_size=1000)\n",
    "\n",
    "    # Fit the k-means model to the descriptors\n",
    "    kmeans.fit(descriptors)\n",
    "\n",
    "    # Get the cluster centers (codebook)\n",
    "    codebook = kmeans.cluster_centers_\n",
    "\n",
    "    return codebook\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:20:08.317641Z",
     "start_time": "2023-12-06T16:20:08.218065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def assignments(descriptors, clusters):\n",
    "    \"\"\"\n",
    "    compute assignment matrix\n",
    "    parameters:\n",
    "        descriptors: TxD descriptor matrix\n",
    "        clusters: KxD cluster matrix\n",
    "    returns: TxK assignment matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute nearest neighbors\n",
    "    nearest_cluster_indices = clusters.predict(descriptors)\n",
    "\n",
    "    # Create hard assignment matrix using one-hot encoding\n",
    "    assignment = np.zeros((len(descriptors), len(clusters)))\n",
    "    assignment[np.arange(len(descriptors)), nearest_cluster_indices] = 1\n",
    "\n",
    "    return assignment"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:20:09.095279Z",
     "start_time": "2023-12-06T16:20:09.088431Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def vlad(files, mus, powernorm, gmp=False, gamma=1000):\n",
    "    \"\"\"\n",
    "    compute VLAD encoding for each files\n",
    "    parameters:\n",
    "        files: list of N files containing each T local descriptors of dimension\n",
    "        D\n",
    "        mus: KxD matrix of cluster centers\n",
    "        gmp: if set to True use generalized max pooling instead of sum pooling\n",
    "    returns: NxK*D matrix of encodings\n",
    "    \"\"\"\n",
    "    K = mus.shape[0]\n",
    "    encodings = []\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        with gzip.open(f, 'rb') as ff:\n",
    "            desc = cPickle.load(ff, encoding='latin1')\n",
    "        a = assignments(desc, mus)\n",
    "\n",
    "        T,D = desc.shape\n",
    "        f_enc = np.zeros( (D*K), dtype=np.float32)\n",
    "        for k in range(mus.shape[0]):\n",
    "            cluster_assigned_indices = a[:, k].astype(bool)\n",
    "            cluster_descriptors = desc[cluster_assigned_indices, :]\n",
    "\n",
    "            # Compute residuals\n",
    "            residuals = cluster_descriptors - mus[k, :]\n",
    "\n",
    "            # Flatten and aggregate residuals\n",
    "            f_enc[k * D:(k + 1) * D] = residuals.flatten().sum(axis=0)\n",
    "\n",
    "        # c) power normalization\n",
    "        if powernorm:\n",
    "            f_enc = np.sign(f_enc) * np.abs(f_enc) ** 0.5\n",
    "\n",
    "        # l2 normalization\n",
    "        f_enc = normalize(f_enc.reshape(1, -1), norm='l2').flatten()\n",
    "\n",
    "        encodings.append(f_enc)\n",
    "\n",
    "    return np.vstack(encodings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:47:13.590475Z",
     "start_time": "2023-12-10T14:47:13.576102Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (182201320.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[2], line 39\u001B[0;36m\u001B[0m\n\u001B[0;31m    new_encs = list(tqdm(executor.map(lambda i: loop(i, encs_test, encs_train, C), range(len(encs_test))), total=len(encs_test)))\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def esvm(encs_test, encs_train, C=1000):\n",
    "    \"\"\"\n",
    "    compute a new embedding using Exemplar Classification\n",
    "    compute for each encs_test encoding an E-SVM using the\n",
    "    encs_train as negatives\n",
    "    parameters:\n",
    "        encs_test: NxD matrix\n",
    "        encs_train: MxD matrix\n",
    "\n",
    "    returns: new encs_test matrix (NxD)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    # Set up labels\n",
    "    labels = np.zeros(len(encs_train) + len(encs_test))\n",
    "    labels[len(encs_train):] = 1\n",
    "\n",
    "    def loop(i):\n",
    "        # Extract the i-th test encoding\n",
    "        x_test = encs_test[i].reshape(1, -1)\n",
    "\n",
    "        # Combine test and train for SVM training\n",
    "        X_combined = np.concatenate([encs_train, x_test], axis=0)\n",
    "\n",
    "        # Set up labels for training\n",
    "        y_train = np.zeros(len(X_combined))\n",
    "        y_train[:len(encs_train)] = -1\n",
    "\n",
    "        # Train LinearSVC\n",
    "        clf = LinearSVC(C=C, class_weight='balanced')\n",
    "        clf.fit(X_combined, y_train)\n",
    "\n",
    "        # Feature transformation\n",
    "        x_transformed = clf.coef_.reshape(1, -1)\n",
    "\n",
    "        return x_transformed\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        new_encs = list(tqdm(executor.map(lambda i: loop(i, encs_test, encs_train, C), range(len(encs_test))), total=len(encs_test)))\n",
    "\n",
    "    # Concatenate the results\n",
    "    new_encs = np.concatenate(new_encs, axis=0)\n",
    "    return new_encs\n",
    "\n",
    "    # testing and trainning data\n",
    "\n",
    "    # labels_train = np.zeros(encs_train.shape[0])\n",
    "    # labels_test = np.ones(encs_test.shape[0])\n",
    "    # #\n",
    "    # # #encoding train and test for SVM training\n",
    "    # encodings = np.concatenate((encs_train,encs_test), axis =0)\n",
    "    # labels = np.concatenate((labels_train,labels_test))\n",
    "    # #\n",
    "    # # #SVM classifier\n",
    "    # svm = LinearSVC(C=C)\n",
    "    # svm.fit(encodings,labels)\n",
    "    #\n",
    "    # # set up labels\n",
    "    # new_labels = svm.predict(encs_test)\n",
    "    #\n",
    "    # new_encs_test = encs_test[new_labels == 1]\n",
    "    #\n",
    "    # return encs_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:47:16.149191Z",
     "start_time": "2023-12-10T14:47:16.027828Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def distances(encs):\n",
    "    \"\"\"\n",
    "    compute pairwise distances\n",
    "\n",
    "    parameters:\n",
    "        encs:  TxK*D encoding matrix\n",
    "    returns: TxT distance matrix\n",
    "    \"\"\"\n",
    "    # compute cosine distance = 1 - dot product between l2-normalized\n",
    "    # encodings\n",
    "    # TODO\n",
    "      # Compute cosine distance = 1 - dot product between l2-normalized\n",
    "    # encodings\n",
    "    norm_encs = normalize(encs, norm='l2', axis=1)  # L2-normalize the encodings\n",
    "    dists = 1 - np.dot(norm_encs, norm_encs.T)\n",
    "\n",
    "    # Mask out distance with itself\n",
    "    np.fill_diagonal(dists, np.finfo(dists.dtype).max)\n",
    "\n",
    "    return dists"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:20:12.009280Z",
     "start_time": "2023-12-06T16:20:11.982481Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def evaluate(encs, labels):\n",
    "    \"\"\"\n",
    "    evaluate encodings assuming using associated labels\n",
    "    parameters:\n",
    "        encs: TxK*D encoding matrix\n",
    "        labels: array/list of T labels\n",
    "    \"\"\"\n",
    "    dist_matrix = distances(encs)\n",
    "    # sort each row of the distance matrix\n",
    "    indices = dist_matrix.argsort()\n",
    "\n",
    "    n_encs = len(encs)\n",
    "\n",
    "    mAP = []\n",
    "    correct = 0\n",
    "    for r in range(n_encs):\n",
    "        precisions = []\n",
    "        rel = 0\n",
    "        for k in range(n_encs-1):\n",
    "            if labels[ indices[r,k] ] == labels[ r ]:\n",
    "                rel += 1\n",
    "                precisions.append( rel / float(k+1) )\n",
    "                if k == 0:\n",
    "                    correct += 1\n",
    "        avg_precision = np.mean(precisions)\n",
    "        mAP.append(avg_precision)\n",
    "    mAP = np.mean(mAP)\n",
    "\n",
    "    print('Top-1 accuracy: {} - mAP: {}'.format(float(correct) / n_encs, mAP))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:20:12.874201Z",
     "start_time": "2023-12-06T16:20:12.865032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 1182\n",
      "> compute VLAD for test\n",
      "#test: 3600\n",
      "> evaluate\n",
      "Top-1 accuracy: 0.6561111111111111 - mAP: 0.45644080789869945\n",
      "> compute VLAD for train (for E-SVM)\n",
      "> E-SVM computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3600 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'spawn.<locals>.fun'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 73\u001B[0m\n\u001B[1;32m     70\u001B[0m     cPickle\u001B[38;5;241m.\u001B[39mdump(enc_train, fOut, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m> E-SVM computation\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 73\u001B[0m new_enc_test \u001B[38;5;241m=\u001B[39m \u001B[43mesvm\u001B[49m\u001B[43m(\u001B[49m\u001B[43menc_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Save intermediate results for debugging\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m gzip\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnew_enc_test_intermediate.pkl.gz\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fOut:\n",
      "Cell \u001B[0;32mIn[26], line 38\u001B[0m, in \u001B[0;36mesvm\u001B[0;34m(encs_test, encs_train, C)\u001B[0m\n\u001B[1;32m     34\u001B[0m     x_transformed \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mcoef_\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x_transformed\n\u001B[0;32m---> 38\u001B[0m new_encs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mparmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mloop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mencs_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     39\u001B[0m new_encs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(new_encs, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# return new encodings\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[18], line 36\u001B[0m, in \u001B[0;36mparmap\u001B[0;34m(f, iterable, nprocs, show_progress, size)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m proc:\n\u001B[1;32m     35\u001B[0m     p\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m     \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m progress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m     progress\u001B[38;5;241m.\u001B[39mstart()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:284\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_launch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:47\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(prep_data, fp)\n\u001B[0;32m---> 47\u001B[0m     \u001B[43mreduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[0;34m(obj, file, protocol)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdump\u001B[39m(obj, file, protocol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m     \u001B[43mForkingPickler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Can't pickle local object 'spawn.<locals>.fun'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = argparse.Namespace(\n",
    "        labels_test='/Users/pankajrathi/Projcv/exercise3/icdar17_local_features/icdar17_labels_test.txt',\n",
    "        labels_train='/Users/pankajrathi/Projcv/exercise3/icdar17_local_features/icdar17_labels_train.txt',\n",
    "        suffix='_SIFT_patch_pr.pkl.gz',\n",
    "        in_test='/Users/pankajrathi/Projcv/exercise3/icdar17_local_features/test',\n",
    "        in_train='/Users/pankajrathi/Projcv/exercise3/icdar17_local_features/train',\n",
    "        overwrite=False,\n",
    "        powernorm=False,\n",
    "        gmp=False,\n",
    "        gamma=1,\n",
    "        C=1000,\n",
    "        n_clusters = 32\n",
    "    )\n",
    "    # parser = argparse.ArgumentParser('retrieval')\n",
    "    # parser = parseArgs(parser)\n",
    "    # args = parser.parse_args()\n",
    "    # np.random.seed(42)  # fix random seed\n",
    "\n",
    "    # a) dictionary\n",
    "    files_train, labels_train = getFiles(args.in_train, args.suffix, args.labels_train)\n",
    "    print('#train: {}'.format(len(files_train)))\n",
    "    if not os.path.exists('mus.pkl.gz'):\n",
    "        descriptors = []\n",
    "        for f in tqdm(files_train):\n",
    "            with gzip.open(f, 'rb') as ff:\n",
    "                desc = cPickle.load(ff, encoding='latin1')\n",
    "                descriptors.extend(desc)\n",
    "\n",
    "        # Convert descriptors to a numpy array\n",
    "        descriptors = np.vstack(descriptors)\n",
    "        mus = dictionary(descriptors[np.random.choice(descriptors.shape[0], 500000, replace=False)], args.n_clusters)\n",
    "\n",
    "        with gzip.open('mus.pkl.gz', 'wb') as fOut:\n",
    "            cPickle.dump(mus, fOut, -1)\n",
    "    else:\n",
    "        with gzip.open('mus.pkl.gz', 'rb') as f:\n",
    "            mus = cPickle.load(f)\n",
    "\n",
    "    # b) VLAD encoding\n",
    "    print('> compute VLAD for test')\n",
    "    files_test, labels_test = getFiles(args.in_test, args.suffix, args.labels_test)\n",
    "    print('#test: {}'.format(len(files_test)))\n",
    "    fname = 'enc_test_gmp{}.pkl.gz'.format(args.gamma) if args.gmp else 'enc_test.pkl.gz'\n",
    "    if not os.path.exists(fname) or args.overwrite:\n",
    "        enc_test = vlad(files_test, mus, powernorm=args.powernorm, gmp=args.gmp, gamma=args.gamma)\n",
    "        with gzip.open(fname, 'wb') as fOut:\n",
    "            cPickle.dump(enc_test, fOut, -1)\n",
    "    else:\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            enc_test = cPickle.load(f)\n",
    "\n",
    "    # Cross-evaluate test encodings\n",
    "    print('> evaluate')\n",
    "    evaluate(enc_test, labels_test)\n",
    "\n",
    "    # d) Compute exemplar SVMs\n",
    "    print('> compute VLAD for train (for E-SVM)')\n",
    "    fname = 'enc_train_gmp{}.pkl.gz'.format(args.gamma) if args.gmp else 'enc_train.pkl.gz'\n",
    "    if not os.path.exists(fname) or args.overwrite:\n",
    "        enc_train = vlad(files_train, mus, powernorm=args.powernorm, gmp=args.gmp, gamma=args.gamma)\n",
    "        with gzip.open(fname, 'wb') as fOut:\n",
    "            cPickle.dump(enc_train, fOut, -1)\n",
    "    else:\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            enc_train = cPickle.load(f)\n",
    "\n",
    "    # Save intermediate results for debugging\n",
    "    with gzip.open('enc_train_intermediate.pkl.gz', 'wb') as fOut:\n",
    "        cPickle.dump(enc_train, fOut, -1)\n",
    "\n",
    "    print('> E-SVM computation')\n",
    "    new_enc_test = esvm(enc_test, enc_train, args.C)\n",
    "\n",
    "    # Save intermediate results for debugging\n",
    "    with gzip.open('new_enc_test_intermediate.pkl.gz', 'wb') as fOut:\n",
    "        cPickle.dump(new_enc_test, fOut, -1)\n",
    "\n",
    "    # new_enc_test = np.array(parmap(enc_test, enc_train, args.C))\n",
    "    # Eval\n",
    "\n",
    "    evaluate(new_enc_test, labels_test)\n",
    "    print('> evaluate after E-SVM')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T16:20:19.125011Z",
     "start_time": "2023-12-06T16:20:13.849001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-06T16:19:47.970456Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
